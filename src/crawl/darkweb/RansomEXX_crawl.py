import aiohttp
import asyncio
from bs4 import BeautifulSoup
import aiohttp_socks  # SOCKS5 í”„ë¡ì‹œ ì§€ì› íŒ¨í‚¤ì§€

# 1. í¬ë¡¤ë§í•  .onion ì‚¬ì´íŠ¸ URL
url = "http://rnsm777cdsjrsdlbs4v5qoeppu3px6sb2igmh53jzrx7ipcrbjz5b2ad.onion/"

# 2. HTTP ìš”ì²­ í—¤ë” ì„¤ì •
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}

# 3. SOCKS5 í”„ë¡ì‹œ ì„¤ì • (Tor ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©)
proxy_url = "socks5://127.0.0.1:9050"

async def crawl_darkweb():
    """
    ğŸ•µï¸â€â™‚ï¸ ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë‹¤í¬ì›¹ í¬ë¡¤ë§ í›„ ë°ì´í„° ë°˜í™˜
    - í˜ì´ì§€ ìš”ì²­ í›„ HTML íŒŒì‹±í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
    - ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ì¶œë ¥ ì—†ìŒ)
    - í¬ë¡¤ë§ì´ ëë‚˜ë©´ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
    """
    connector = aiohttp_socks.ProxyConnector.from_url(proxy_url)

    try:
        async with aiohttp.ClientSession(connector=connector) as session:
            async with session.get(url, headers=headers, timeout=30) as response:
                response.raise_for_status()
                html = await response.text()
    except aiohttp.ClientError:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜")  # ğŸš¨ ì‹¤íŒ¨ ì‹œ ë©”ì‹œì§€ ì¶œë ¥
        return []  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    # ğŸ”¹ HTML íŒŒì‹±
    soup = BeautifulSoup(html, "html.parser")
    leak_data = []

    # ğŸ”¹ ol íƒœê·¸ ë‚´ì˜ li íƒœê·¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
    for ol in soup.find_all("ol"):
        for li in ol.find_all("li"):
            title = li.find("h4").get_text(strip=True) if li.find("h4") else "ì œëª© ì—†ìŒ"
            description = li.find("p").get_text(strip=True) if li.find("p") else "ìœ ì¶œ ì •ë³´ ì—†ìŒ"

            # ë°ì´í„° ì €ì¥ (ì¶œë ¥ ì—†ì´ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥)
            leak_data.append({"title": title, "description": description})

    print(f"ğŸ” {len(leak_data)}ê°œì˜ ë°ì´í„°ê°€ í¬ë¡¤ë§ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return leak_data  # âœ… ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ (ì¶œë ¥ ì—†ìŒ)

# í”„ë¡œê·¸ë¨ ì‹¤í–‰ë¶€ (í…ŒìŠ¤íŠ¸ìš©, í•„ìš” ì—†ìœ¼ë©´ ì œê±° ê°€ëŠ¥)
if __name__ == "__main__":
    async def main():
        data = await crawl_darkweb()  # âœ… í¬ë¡¤ë§ ì‹¤í–‰ í›„ ë°ì´í„° ì €ì¥
        return data

    asyncio.run(main())  # âœ… ë¹„ë™ê¸° ì‹¤í–‰
